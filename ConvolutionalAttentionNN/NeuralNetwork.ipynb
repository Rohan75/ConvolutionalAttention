{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "NeuralNetwork.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.7"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8o9J3M2maKoY",
        "outputId": "78e79e13-a1e8-4f54-81bb-005ec0fff5a8"
      },
      "source": [
        "%pip install logomaker"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: logomaker in c:\\users\\ghotr\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (0.8)\n",
            "Requirement already satisfied: pandas in c:\\users\\ghotr\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from logomaker) (1.2.0)\n",
            "Requirement already satisfied: matplotlib in c:\\users\\ghotr\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from logomaker) (3.3.3)\n",
            "Requirement already satisfied: numpy in c:\\users\\ghotr\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from logomaker) (1.19.4)\n",
            "Requirement already satisfied: cycler>=0.10 in c:\\users\\ghotr\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from matplotlib->logomaker) (0.10.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\ghotr\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from matplotlib->logomaker) (8.1.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\ghotr\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from matplotlib->logomaker) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\users\\ghotr\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from matplotlib->logomaker) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\ghotr\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from matplotlib->logomaker) (2.8.1)\n",
            "Requirement already satisfied: six in c:\\users\\ghotr\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from cycler>=0.10->matplotlib->logomaker) (1.15.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in c:\\users\\ghotr\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pandas->logomaker) (2020.5)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GwUSIzFCDo0",
        "outputId": "dbeb6eea-98a1-4c6a-a3d6-43bb5d5c8337"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "import numpy as np\n",
        "import requests as rq\n",
        "import io, h5py\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import logomaker\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Matplotlib is building the font cache; this may take a moment.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z40FDKDrCDo3",
        "outputId": "b77452b8-6e78-462b-ea66-e7d56b95e1c1"
      },
      "source": [
        "data = rq.get('https://www.dropbox.com/s/c3umbo5y13sqcfp/synthetic_dataset.h5?raw=true')\n",
        "data.raise_for_status()\n",
        "\n",
        "with h5py.File(io.BytesIO(data.content), 'r') as dataset:\n",
        "    x_train = np.array(dataset['X_train']).astype(np.float32).transpose([0, 2, 1])\n",
        "    y_train = np.array(dataset['Y_train']).astype(np.float32)\n",
        "    x_valid = np.array(dataset['X_valid']).astype(np.float32).transpose([0, 2, 1])\n",
        "    y_valid = np.array(dataset['Y_valid']).astype(np.int32)\n",
        "    x_test = np.array(dataset['X_test']).astype(np.float32).transpose([0, 2, 1])\n",
        "    y_test = np.array(dataset['Y_test']).astype(np.int32)\n",
        "\n",
        "print(x_train.shape, y_train.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(21000, 200, 4) (21000, 12)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccCeov6oObuI"
      },
      "source": [
        "def generate_model(params):\n",
        "  filters, kernel, BN1, pool, stride, heads, key_size, dense, BN2, pos, drop, layerNorm = params\n",
        "  inputs = layers.Input(shape=(200, 4))\n",
        "\n",
        "  nn = layers.Conv1D(filters=filters, kernel_size=kernel, padding='same', use_bias='false')(inputs)\n",
        "  if BN1:\n",
        "    nn = layers.BatchNormalization()(nn)\n",
        "  nn = layers.Activation('relu')(nn)\n",
        "  if pool != 0:\n",
        "    nn = layers.MaxPool1D(pool_size=pool, strides=stride)(nn)\n",
        "  nn = layers.Dropout(0.05)(nn)\n",
        "\n",
        "  if pos:\n",
        "    positions = tf.range(nn.shape[1])\n",
        "    context = layers.Embedding(input_dim=nn.shape[1], output_dim=nn.shape[2])(positions)\n",
        "\n",
        "    contextual_meaning = tf.add(nn, context)\n",
        "  else:\n",
        "    contextual_meaning = nn\n",
        "\n",
        "  attention, weights = layers.MultiHeadAttention(num_heads=heads, key_dim=key_size, dropout=0)(contextual_meaning, contextual_meaning, return_attention_scores=True)\n",
        "  if drop:\n",
        "    nn = layers.Dropout(0.1)(attention)\n",
        "  if layerNorm:\n",
        "    nn = layers.LayerNorm()(nn)\n",
        "\n",
        "  nn = layers.Flatten()(attention)\n",
        "  nn = layers.Dense(dense, use_bias=False)(nn)\n",
        "  if BN2:\n",
        "    nn = layers.BatchNormalization()(nn)\n",
        "  nn = layers.Activation('relu')(nn)\n",
        "  nn = layers.Dropout(0.5)(nn)\n",
        "\n",
        "  output = layers.Dense(12, activation='sigmoid')(nn)\n",
        "\n",
        "  model = Model(inputs=inputs, outputs=output)\n",
        "  model.compile(optimizer=tf.keras.optimizers.Adam(0.0001), loss='binary_crossentropy', metrics=[tf.keras.metrics.AUC(curve='ROC', name='auroc'), tf.keras.metrics.AUC(curve='PR', name='aupr')])\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JAqmnFAnCDo6"
      },
      "source": [
        "base = [64, 19, True, 4, 4, 8, 32, 512, True, False, True, True]\n",
        "\n",
        "filters = [4, 8, 16, 32, 64, 96, 128]\n",
        "kernels = [19]\n",
        "BN1 = [True, False]\n",
        "pools = [0, 2, 4, 6, 10, 20, 25, 50, 100]\n",
        "strides= [1, 2, 4, 6, 10]\n",
        "heads = [1, 2, 4, 8, 12, 20]\n",
        "key_sizes = [32, 64, 128, 256]\n",
        "denses = [32, 64, 128, 256, 512]\n",
        "BN2 = [True, False]\n",
        "\n",
        "model = generate_model(base)\n",
        "\n",
        "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20, verbose=1, mode='min', restore_best_weights=False)\n",
        "lr_decay = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-10, mode='min', verbose=1) \n",
        "model.fit(x_train, y_train, epochs=2, validation_data=(x_valid, y_valid), callbacks=[early_stop, lr_decay])\n",
        "\n",
        "model.evaluate(x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DudKVy_TrKA1"
      },
      "source": [
        "layer = 3        # activation layer for 1st convolutional layer\n",
        "threshold = 0.5  # threshold for significant activations\n",
        "window = 20      # window size of alignment \n",
        "\n",
        "# get feature maps of 1st convolutional layer after activation\n",
        "intermediate = tf.keras.Model(inputs=model.inputs, outputs=model.layers[layer].output)\n",
        "fmap = intermediate.predict(x_test)\n",
        "num_filters = fmap.shape[-1]\n",
        "\n",
        "# set the left and right window sizes\n",
        "window_left = int(window/2)\n",
        "window_right = window - window_left\n",
        "\n",
        "N, L, A = x_test.shape\n",
        "\n",
        "W = []\n",
        "for filter_index in range(num_filters):\n",
        "\n",
        "    # find regions above threshold\n",
        "    coords = np.where(fmap[:,:,filter_index] > np.max(fmap[:,:,filter_index])*threshold)\n",
        "    x, y = coords\n",
        "\n",
        "    # sort score\n",
        "    index = np.argsort(fmap[x,y,filter_index])[::-1]\n",
        "    data_index = x[index].astype(int)\n",
        "    pos_index = y[index].astype(int)\n",
        "\n",
        "    # make a sequence alignment centered about each activation (above threshold)\n",
        "    seq_align = []\n",
        "    for i in range(len(pos_index)):\n",
        "\n",
        "        # determine position of window about each filter activation\n",
        "        start_window = pos_index[i] - window_left\n",
        "        end_window = pos_index[i] + window_right\n",
        "\n",
        "        # check to make sure positions are valid\n",
        "        if (start_window > 0) & (end_window < L):\n",
        "            seq = x_test[data_index[i], start_window:end_window, :]\n",
        "            seq_align.append(seq)\n",
        "\n",
        "    # calculate position probability matrix\n",
        "    W.append(np.mean(seq_align, axis=0))\n",
        "W = np.array(W)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPkYm5BNrKA3"
      },
      "source": [
        "fig = plt.figure(figsize=(30,5))\n",
        "fig.subplots_adjust(hspace=0.1, wspace=0.1)\n",
        "\n",
        "num_cols = 8\n",
        "num_widths = int(np.ceil(num_filters/num_cols))\n",
        "for n, w in enumerate(W):\n",
        "    ax = fig.add_subplot(num_widths, num_cols, n+1)\n",
        "    \n",
        "    if not np.isnan(w).any():\n",
        "        # calculate sequence logo heights -- information\n",
        "        I = np.log2(4) + np.sum(w * np.log2(w+1e-7), axis=1, keepdims=True)\n",
        "        logo = I*w\n",
        "\n",
        "        # create dataframe for logomaker\n",
        "        filter_len = w.shape[0]\n",
        "        counts_df = pd.DataFrame(data=0.0, columns=list('ACGT'), index=list(range(filter_len)))\n",
        "        for a in range(A):\n",
        "            for l in range(filter_len):\n",
        "                counts_df.iloc[l,a] = logo[l,a]\n",
        "\n",
        "        # plot filter representation\n",
        "        logomaker.Logo(counts_df, ax=ax)\n",
        "        ax = plt.gca()\n",
        "        ax.spines['right'].set_visible(False)\n",
        "        ax.spines['top'].set_visible(False)\n",
        "        ax.yaxis.set_ticks_position('none')\n",
        "        ax.xaxis.set_ticks_position('none')\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}