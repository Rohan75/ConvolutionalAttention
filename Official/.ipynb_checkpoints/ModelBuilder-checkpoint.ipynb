{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HNwXeEFCjLjU"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "cORRd0BKjLjW"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "import numpy as np\n",
    "import requests as rq\n",
    "import io, h5py\n",
    "import pickle as pk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NP5SXKhMjLjX"
   },
   "source": [
    "# Retrieve Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HOVTZwAPjLjX",
    "outputId": "60319f52-beec-49dc-ac2e-b1fa61f5b8f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21000, 200, 4) (21000, 12)\n"
     ]
    }
   ],
   "source": [
    "data = rq.get('https://www.dropbox.com/s/c3umbo5y13sqcfp/synthetic_dataset.h5?raw=true')\n",
    "data.raise_for_status()\n",
    "\n",
    "with h5py.File(io.BytesIO(data.content), 'r') as dataset:\n",
    "    x_train = np.array(dataset['X_train']).astype(np.float32).transpose([0, 2, 1])\n",
    "    y_train = np.array(dataset['Y_train']).astype(np.float32)\n",
    "    x_valid = np.array(dataset['X_valid']).astype(np.float32).transpose([0, 2, 1])\n",
    "    y_valid = np.array(dataset['Y_valid']).astype(np.int32)\n",
    "    x_test = np.array(dataset['X_test']).astype(np.float32).transpose([0, 2, 1])\n",
    "    y_test = np.array(dataset['Y_test']).astype(np.int32)\n",
    "\n",
    "print(x_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fq8SRFhPjLjY"
   },
   "source": [
    "# Connect to Drive (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kJ1WgkFPjLjY",
    "outputId": "322ae112-415a-4be5-90ad-e6efd6609582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I2_xcPd5jLjZ"
   },
   "source": [
    "# Construct Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "DF1C8iHGjLja"
   },
   "outputs": [],
   "source": [
    "def build_model(params):\n",
    "    # Hyperparameters\n",
    "    filters, kernel_size, batch_norm_1, pool_size, dropout_1, pos_encoding, heads, key_size, dropout_2, dense_size, batch_norm_2, dropout_3 = params\n",
    "    \n",
    "    # Input Layer\n",
    "    inputs = layers.Input(shape=(200, 4))\n",
    "    \n",
    "    # Convolution Block\n",
    "    nn = layers.Conv1D(filters=filters, kernel_size=kernel_size, use_bias=False, padding='same')(inputs)\n",
    "    if batch_norm_1:\n",
    "        nn = layers.BatchNormalization()(nn)\n",
    "    nn = layers.Activation('relu')(nn)\n",
    "    nn = layers.MaxPool1D(pool_size=pool_size)(nn)\n",
    "    if dropout_1 != 0:\n",
    "        nn = layers.Dropout(dropout_1)(nn)\n",
    "    \n",
    "    # Multi-Head Attention Block\n",
    "    if pos_encoding:\n",
    "        positions = tf.range(nn.shape[1])\n",
    "        context = layers.Embedding(input_dim=nn.shape[1], output_dim=nn.shape[2])(positions)\n",
    "        nn = tf.add(nn, context)  # contextual meaning\n",
    "\n",
    "    attention, weights = layers.MultiHeadAttention(num_heads=heads, key_dim=key_size)(nn, nn, return_attention_scores=True)\n",
    "    if dropout_2 != 0:\n",
    "        nn = layers.Dropout(dropout_2)(attention)\n",
    "    nn = layers.LayerNormalization()(nn)\n",
    "    \n",
    "    # Dense Block\n",
    "    nn = layers.Flatten()(nn)\n",
    "    nn = layers.Dense(dense_size, use_bias=False)(nn)\n",
    "    if batch_norm_2:\n",
    "        nn = layers.BatchNormalization()(nn)\n",
    "    nn = layers.Activation('relu')(nn)\n",
    "    if dropout_3 != 0:\n",
    "        nn = layers.Dropout(dropout_3)(nn)\n",
    "    \n",
    "    # Outputs\n",
    "    outputs = layers.Dense(12, activation='sigmoid')(nn)\n",
    "    \n",
    "    # Build Model\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(0.0005), loss='binary_crossentropy', metrics=[tf.keras.metrics.AUC(curve='ROC', name='auroc'), tf.keras.metrics.AUC(curve='PR', name='aupr')])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "zGqDkh8CjLjb",
    "outputId": "16cfd45f-7d5b-4dc1-c38e-7db326a3cc14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/75\n",
      "657/657 [==============================] - 8s 9ms/step - loss: 0.5175 - auroc: 0.5272 - aupr: 0.1633 - val_loss: 0.5986 - val_auroc: 0.6140 - val_aupr: 0.2706\n",
      "Epoch 2/75\n",
      "657/657 [==============================] - 5s 8ms/step - loss: 0.3650 - auroc: 0.7152 - aupr: 0.4162 - val_loss: 0.3270 - val_auroc: 0.8078 - val_aupr: 0.5562\n",
      "Epoch 3/75\n",
      "657/657 [==============================] - 5s 8ms/step - loss: 0.3089 - auroc: 0.8118 - aupr: 0.5564 - val_loss: 0.2760 - val_auroc: 0.8759 - val_aupr: 0.6493\n",
      "Epoch 4/75\n",
      "657/657 [==============================] - 5s 8ms/step - loss: 0.2720 - auroc: 0.8642 - aupr: 0.6560 - val_loss: 0.2506 - val_auroc: 0.9037 - val_aupr: 0.7231\n",
      "Epoch 5/75\n",
      "657/657 [==============================] - 6s 8ms/step - loss: 0.2591 - auroc: 0.8796 - aupr: 0.6852 - val_loss: 0.2425 - val_auroc: 0.9121 - val_aupr: 0.7367\n",
      "Epoch 6/75\n",
      "657/657 [==============================] - 5s 8ms/step - loss: 0.2511 - auroc: 0.8887 - aupr: 0.7042 - val_loss: 0.2441 - val_auroc: 0.9115 - val_aupr: 0.7255\n",
      "Epoch 7/75\n",
      "657/657 [==============================] - 5s 8ms/step - loss: 0.2404 - auroc: 0.8999 - aupr: 0.7278 - val_loss: 0.2268 - val_auroc: 0.9260 - val_aupr: 0.7670\n",
      "Epoch 8/75\n",
      "657/657 [==============================] - 6s 8ms/step - loss: 0.2347 - auroc: 0.9050 - aupr: 0.7374 - val_loss: 0.2206 - val_auroc: 0.9300 - val_aupr: 0.7758\n",
      "Epoch 9/75\n",
      "657/657 [==============================] - 5s 8ms/step - loss: 0.2282 - auroc: 0.9113 - aupr: 0.7507 - val_loss: 0.2464 - val_auroc: 0.9033 - val_aupr: 0.7477\n",
      "Epoch 10/75\n",
      "657/657 [==============================] - 6s 8ms/step - loss: 0.2266 - auroc: 0.9122 - aupr: 0.7542 - val_loss: 0.2185 - val_auroc: 0.9387 - val_aupr: 0.7984\n",
      "Epoch 11/75\n",
      "657/657 [==============================] - 6s 9ms/step - loss: 0.2178 - auroc: 0.9196 - aupr: 0.7697 - val_loss: 0.2021 - val_auroc: 0.9390 - val_aupr: 0.8089\n",
      "Epoch 12/75\n",
      "657/657 [==============================] - 6s 9ms/step - loss: 0.2124 - auroc: 0.9236 - aupr: 0.7811 - val_loss: 0.2033 - val_auroc: 0.9459 - val_aupr: 0.8230\n",
      "Epoch 13/75\n",
      "657/657 [==============================] - 5s 8ms/step - loss: 0.2097 - auroc: 0.9250 - aupr: 0.7878 - val_loss: 0.2049 - val_auroc: 0.9331 - val_aupr: 0.8058\n",
      "Epoch 14/75\n",
      "657/657 [==============================] - 6s 8ms/step - loss: 0.2072 - auroc: 0.9256 - aupr: 0.7923 - val_loss: 0.2055 - val_auroc: 0.9386 - val_aupr: 0.8028\n",
      "Epoch 15/75\n",
      "657/657 [==============================] - 5s 8ms/step - loss: 0.2007 - auroc: 0.9322 - aupr: 0.8008 - val_loss: 0.1926 - val_auroc: 0.9491 - val_aupr: 0.8328\n",
      "Epoch 16/75\n",
      "657/657 [==============================] - 6s 8ms/step - loss: 0.2008 - auroc: 0.9323 - aupr: 0.8030 - val_loss: 0.1811 - val_auroc: 0.9559 - val_aupr: 0.8501\n",
      "Epoch 17/75\n",
      "657/657 [==============================] - 6s 9ms/step - loss: 0.1979 - auroc: 0.9342 - aupr: 0.8071 - val_loss: 0.1901 - val_auroc: 0.9521 - val_aupr: 0.8403\n",
      "Epoch 18/75\n",
      "657/657 [==============================] - 5s 8ms/step - loss: 0.1947 - auroc: 0.9365 - aupr: 0.8115 - val_loss: 0.1964 - val_auroc: 0.9485 - val_aupr: 0.8307\n",
      "Epoch 19/75\n",
      "657/657 [==============================] - 6s 8ms/step - loss: 0.1940 - auroc: 0.9371 - aupr: 0.8127 - val_loss: 0.1981 - val_auroc: 0.9435 - val_aupr: 0.8104\n",
      "Epoch 20/75\n",
      "657/657 [==============================] - 6s 8ms/step - loss: 0.1911 - auroc: 0.9380 - aupr: 0.8171 - val_loss: 0.1823 - val_auroc: 0.9526 - val_aupr: 0.8456\n",
      "Epoch 21/75\n",
      "657/657 [==============================] - 6s 8ms/step - loss: 0.1889 - auroc: 0.9402 - aupr: 0.8199 - val_loss: 0.1836 - val_auroc: 0.9546 - val_aupr: 0.8456\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 22/75\n",
      "657/657 [==============================] - 6s 8ms/step - loss: 0.1870 - auroc: 0.9419 - aupr: 0.8278 - val_loss: 0.1733 - val_auroc: 0.9576 - val_aupr: 0.8582\n",
      "Epoch 23/75\n",
      "657/657 [==============================] - 6s 8ms/step - loss: 0.1839 - auroc: 0.9430 - aupr: 0.8317 - val_loss: 0.1755 - val_auroc: 0.9563 - val_aupr: 0.8534\n",
      "Epoch 24/75\n",
      "657/657 [==============================] - 5s 8ms/step - loss: 0.1817 - auroc: 0.9445 - aupr: 0.8357 - val_loss: 0.1750 - val_auroc: 0.9577 - val_aupr: 0.8569\n",
      "Epoch 25/75\n",
      "657/657 [==============================] - 6s 8ms/step - loss: 0.1777 - auroc: 0.9477 - aupr: 0.8401 - val_loss: 0.1762 - val_auroc: 0.9580 - val_aupr: 0.8558\n",
      "Epoch 26/75\n",
      "657/657 [==============================] - 5s 8ms/step - loss: 0.1792 - auroc: 0.9459 - aupr: 0.8375 - val_loss: 0.1757 - val_auroc: 0.9571 - val_aupr: 0.8553\n",
      "Epoch 27/75\n",
      "657/657 [==============================] - 6s 8ms/step - loss: 0.1779 - auroc: 0.9464 - aupr: 0.8390 - val_loss: 0.1725 - val_auroc: 0.9577 - val_aupr: 0.8589\n",
      "Epoch 28/75\n",
      "657/657 [==============================] - 5s 8ms/step - loss: 0.1764 - auroc: 0.9481 - aupr: 0.8423 - val_loss: 0.1736 - val_auroc: 0.9558 - val_aupr: 0.8563\n",
      "Epoch 29/75\n",
      "657/657 [==============================] - 5s 8ms/step - loss: 0.1770 - auroc: 0.9475 - aupr: 0.8421 - val_loss: 0.1709 - val_auroc: 0.9592 - val_aupr: 0.8605\n",
      "Epoch 30/75\n",
      "657/657 [==============================] - 5s 8ms/step - loss: 0.1781 - auroc: 0.9470 - aupr: 0.8404 - val_loss: 0.1735 - val_auroc: 0.9570 - val_aupr: 0.8554\n",
      "Epoch 31/75\n",
      "657/657 [==============================] - 6s 8ms/step - loss: 0.1761 - auroc: 0.9481 - aupr: 0.8425 - val_loss: 0.1703 - val_auroc: 0.9586 - val_aupr: 0.8609\n",
      "Epoch 32/75\n",
      "657/657 [==============================] - 5s 8ms/step - loss: 0.1774 - auroc: 0.9476 - aupr: 0.8412 - val_loss: 0.1743 - val_auroc: 0.9576 - val_aupr: 0.8578\n",
      "Epoch 33/75\n",
      "657/657 [==============================] - 5s 8ms/step - loss: 0.1742 - auroc: 0.9498 - aupr: 0.8463 - val_loss: 0.1761 - val_auroc: 0.9571 - val_aupr: 0.8534\n",
      "Epoch 34/75\n",
      "657/657 [==============================] - 6s 8ms/step - loss: 0.1733 - auroc: 0.9504 - aupr: 0.8473 - val_loss: 0.1716 - val_auroc: 0.9583 - val_aupr: 0.8606\n",
      "Epoch 35/75\n",
      "657/657 [==============================] - 5s 8ms/step - loss: 0.1769 - auroc: 0.9476 - aupr: 0.8446 - val_loss: 0.1778 - val_auroc: 0.9554 - val_aupr: 0.8518\n",
      "Epoch 36/75\n",
      "657/657 [==============================] - 6s 8ms/step - loss: 0.1729 - auroc: 0.9509 - aupr: 0.8480 - val_loss: 0.1690 - val_auroc: 0.9590 - val_aupr: 0.8637\n",
      "Epoch 37/75\n",
      "657/657 [==============================] - 5s 8ms/step - loss: 0.1732 - auroc: 0.9499 - aupr: 0.8471 - val_loss: 0.1729 - val_auroc: 0.9584 - val_aupr: 0.8588\n",
      "Epoch 38/75\n",
      "657/657 [==============================] - 6s 8ms/step - loss: 0.1719 - auroc: 0.9512 - aupr: 0.8505 - val_loss: 0.1695 - val_auroc: 0.9602 - val_aupr: 0.8638\n",
      "Epoch 39/75\n",
      "657/657 [==============================] - 6s 8ms/step - loss: 0.1730 - auroc: 0.9507 - aupr: 0.8481 - val_loss: 0.1694 - val_auroc: 0.9601 - val_aupr: 0.8638\n",
      "Epoch 40/75\n",
      "657/657 [==============================] - 6s 8ms/step - loss: 0.1693 - auroc: 0.9521 - aupr: 0.8518 - val_loss: 0.1714 - val_auroc: 0.9592 - val_aupr: 0.8617\n",
      "Epoch 41/75\n",
      "657/657 [==============================] - 6s 8ms/step - loss: 0.1726 - auroc: 0.9507 - aupr: 0.8481 - val_loss: 0.1723 - val_auroc: 0.9584 - val_aupr: 0.8604\n",
      "Epoch 42/75\n",
      "657/657 [==============================] - 5s 8ms/step - loss: 0.1727 - auroc: 0.9509 - aupr: 0.8499 - val_loss: 0.1748 - val_auroc: 0.9567 - val_aupr: 0.8550\n",
      "Epoch 43/75\n",
      "657/657 [==============================] - 5s 8ms/step - loss: 0.1720 - auroc: 0.9508 - aupr: 0.8498 - val_loss: 0.1719 - val_auroc: 0.9567 - val_aupr: 0.8580\n",
      "Epoch 44/75\n",
      "657/657 [==============================] - 6s 8ms/step - loss: 0.1700 - auroc: 0.9523 - aupr: 0.8532 - val_loss: 0.1729 - val_auroc: 0.9576 - val_aupr: 0.8603\n",
      "\n",
      "Epoch 00044: ReduceLROnPlateau reducing learning rate to 2.0000000949949027e-05.\n",
      "Epoch 45/75\n",
      "657/657 [==============================] - 6s 8ms/step - loss: 0.1688 - auroc: 0.9533 - aupr: 0.8538 - val_loss: 0.1699 - val_auroc: 0.9592 - val_aupr: 0.8625\n",
      "Epoch 46/75\n",
      "657/657 [==============================] - 6s 8ms/step - loss: 0.1682 - auroc: 0.9532 - aupr: 0.8557 - val_loss: 0.1706 - val_auroc: 0.9591 - val_aupr: 0.8623\n",
      "Epoch 47/75\n",
      "657/657 [==============================] - 6s 9ms/step - loss: 0.1670 - auroc: 0.9538 - aupr: 0.8563 - val_loss: 0.1721 - val_auroc: 0.9583 - val_aupr: 0.8603\n",
      "Epoch 48/75\n",
      "657/657 [==============================] - 6s 8ms/step - loss: 0.1659 - auroc: 0.9544 - aupr: 0.8577 - val_loss: 0.1716 - val_auroc: 0.9586 - val_aupr: 0.8606\n",
      "Epoch 49/75\n",
      "657/657 [==============================] - 6s 8ms/step - loss: 0.1653 - auroc: 0.9549 - aupr: 0.8591 - val_loss: 0.1706 - val_auroc: 0.9587 - val_aupr: 0.8616\n",
      "\n",
      "Epoch 00049: ReduceLROnPlateau reducing learning rate to 4.000000262749381e-06.\n",
      "Epoch 50/75\n",
      "657/657 [==============================] - 6s 8ms/step - loss: 0.1699 - auroc: 0.9525 - aupr: 0.8545 - val_loss: 0.1699 - val_auroc: 0.9589 - val_aupr: 0.8619\n",
      "Epoch 51/75\n",
      "657/657 [==============================] - 6s 8ms/step - loss: 0.1670 - auroc: 0.9542 - aupr: 0.8581 - val_loss: 0.1708 - val_auroc: 0.9586 - val_aupr: 0.8608\n",
      "Epoch 52/75\n",
      "657/657 [==============================] - 6s 8ms/step - loss: 0.1667 - auroc: 0.9540 - aupr: 0.8581 - val_loss: 0.1714 - val_auroc: 0.9585 - val_aupr: 0.8607\n",
      "Epoch 53/75\n",
      "657/657 [==============================] - 6s 8ms/step - loss: 0.1681 - auroc: 0.9538 - aupr: 0.8564 - val_loss: 0.1712 - val_auroc: 0.9585 - val_aupr: 0.8610\n",
      "Epoch 54/75\n",
      "657/657 [==============================] - 6s 8ms/step - loss: 0.1680 - auroc: 0.9535 - aupr: 0.8543 - val_loss: 0.1708 - val_auroc: 0.9589 - val_aupr: 0.8613\n",
      "\n",
      "Epoch 00054: ReduceLROnPlateau reducing learning rate to 8.000000889296644e-07.\n",
      "Epoch 00054: early stopping\n",
      "188/188 [==============================] - 1s 5ms/step - loss: 0.1723 - auroc: 0.9600 - aupr: 0.8618\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"for i in range(len(params)):\\n    for j in range(len(params[i])):\\n        direc = names[i]\\n        name = f'model-{params[i][j]}'\\n        \\n        args = base1.copy()\\n        args[i] = params[i][j]\\n        \\n        model = build_model(args)\\n        \\n        # Callbacks\\n        early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_aupr', patience=15, verbose=1, mode='max', restore_best_weights=False)\\n        lr_decay = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_aupr', factor=0.2, patience=5, min_lr=1e-7, mode='max', verbose=1) \\n        tensorboard = tf.keras.callbacks.Tensorboard(log_dir=f'/content/drive/MyDrive/ColabNotebooks/ConvAttTests/logs/{direc}/{name}')\\n        checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath=f'/content/drive/MyDrive/ColabNotebooks/ConvAttTets/models/{direc}/{name}')\\n        \\n        model.fit(x=x_train, y=y_train, epochs=10, validation_data=(x_valid, y_valid), callbacks=[early_stop, lr_decay, tensorboard, checkpoint])\""
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filters, kernel_size, batch_norm_1, pool_size, dropout_1, pos_encoding, heads, key_size,\n",
    "# dropout_2, dense_size, batch_norm_2, dropout_3\n",
    "base1 = [32, 19, True, 4, 0.1, True, 8, 64, 0.1, 512, True, 0.5]\n",
    "base2 = [32, 19, True, 4, 0.1, True, 8, 64, 0.1, 512, True, 0.5]\n",
    "\n",
    "# Alternate parameters\n",
    "filters = [32, 128, 256]\n",
    "kernels = [19]\n",
    "batch_norm_1 = [True, False]\n",
    "pools = [1, 4, 10, 20]\n",
    "dropout_1 = [0.1, 0.5]\n",
    "pos_encodings = [True, False]\n",
    "heads = [1, 8, 16]\n",
    "keys = [32, 64, 128, 256]\n",
    "dropout_2 = [0.1, 0.5]\n",
    "denses = [64, 256, 512]\n",
    "batch_norm_2 = [True, False]\n",
    "dropout_3 = [0.1, 0.5]\n",
    "\n",
    "names = ['filters', 'kernels', 'batch_norm_1', 'pools', 'dropout_1', 'pos_encodings', 'heads', 'keys', 'dropout_2', 'denses', 'batch_norm_2', 'dropout_3']\n",
    "params = [filters, kernels, batch_norm_1, pools, dropout_1, pos_encodings, heads, keys, dropout_2, denses, batch_norm_2, dropout_3]\n",
    "\n",
    "for i in range(len(params)):\n",
    "    for j in range(len(params[i])):\n",
    "        direc = names[i]\n",
    "        name = f'model-{params[i][j]}'\n",
    "        \n",
    "        args = base1.copy()\n",
    "        args[i] = params[i][j]\n",
    "        \n",
    "        model = build_model(args)\n",
    "        \n",
    "        # Callbacks\n",
    "        early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_aupr', patience=20, verbose=1, mode='max', restore_best_weights=False)\n",
    "        lr_decay = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_aupr', factor=0.2, patience=5, min_lr=1e-7, mode='max', verbose=1) \n",
    "        tensorboard = tf.keras.callbacks.Tensorboard(log_dir=f'/content/drive/MyDrive/ColabNotebooks/ConvAttTests/logs/{direc}/{name}')\n",
    "        checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath=f'/content/drive/MyDrive/ColabNotebooks/ConvAttTests/models/{direc}/{name}.h5')\n",
    "        \n",
    "        model.fit(x=x_train, y=y_train, epochs=75, validation_data=(x_valid, y_valid), callbacks=[early_stop, lr_decay, tensorboard, checkpoint])\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "ModelBuilder.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
